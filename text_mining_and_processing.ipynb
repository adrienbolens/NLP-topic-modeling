{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining and processing\n",
    "\n",
    "\n",
    "This notebook is split in two parts:\n",
    "- __Text mining__: Documents are created out of text extracted from Wikipedia pages using the wikipedia api and [`Wikipedia-API`](https://pypi.org/project/Wikipedia-API/), a Python wrapper. This is done using custom functions defined in the python script [extrac_wikipedia_data.py](extract_wikipedia_data.py).\n",
    "- __Text processing__: The text of the documents are then processed using [spaCy](https://spacy.io/).\n",
    "    This includes\n",
    "    - tokenization of words,\n",
    "    - cleaning-up, \n",
    "    - lemmatization,\n",
    "    - POS tagging.\n",
    "\n",
    "Finally, a dictionary is created, mapping words to numerical ids, and the documents are converted to a bag-of-words format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# for text mining\n",
    "import wikipediaapi\n",
    "import re\n",
    "wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "from extract_wikipedia_data import get_page_text, get_categorymembers, get_authors\n",
    "\n",
    "# for text processing\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from gensim.models.atmodel import construct_author2doc\n",
    "from gensim import models, corpora \n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining\n",
    "\n",
    "My goal is to apply topic modelling to the content of science-fiction novels in english, using only the information available on wikipedia.\n",
    "\n",
    "Here I gather all wikipedia pages corresponding to science-fiction novels. Wikipedia pages are members of categories, which can be probed via the wikipedia API.\n",
    "\n",
    "I use the category [\"Science_fiction_novels_by_year\"](https://en.wikipedia.org/wiki/Category:Science_fiction_novels_by_year), which seems to include the highest amount of different SF novels. Each of its member is a category iself that contains pages about novels of a given year.\n",
    "\n",
    "- First, I use custom functions written in [extrac_wikipedia_data.py](extract_wikipedia_data.py) to collect all relevant pages.\n",
    "- Then, the text of each page is split into its sections, and only relevant sections are kept. These are sections about the content of the novel (e.g. \"plot\", \"theme\"), rather than metadata about the author or the book.\n",
    "\n",
    "Later, I also extract information about the author of the novel from the wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4851 wikipedia pages (novels) written between 1840 and 2019.\n"
     ]
    }
   ],
   "source": [
    "# Extract members and submembers of the following two categories that are not themselves categories.\n",
    "cat = wiki.page(\"Category:Science_fiction_novels_by_year\")\n",
    "cat_years = [page for page in cat.categorymembers.values()]\n",
    "\n",
    "cat = wiki.page(\"Category:Science_fiction_novels_by_writer\")\n",
    "cat_writers = [page for page in cat.categorymembers.values()]\n",
    "\n",
    "pages_by_year = [\n",
    "    get_categorymembers(c, n_pages_per_level_threshold=1, level=0, max_level=None, verbose=False)\n",
    "    for c in cat_years\n",
    "]\n",
    "\n",
    "pages_by_writer = [\n",
    "    get_categorymembers(c, n_pages_per_level_threshold=1, level=0, max_level=None, verbose=False)\n",
    "    for c in cat_writers\n",
    "]\n",
    "\n",
    "pages_years = [p for page_list in pages_by_year for p in page_list]\n",
    "pages_writers = [p for page_list in pages_by_writer for p in page_list]\n",
    "\n",
    "pages_all = pages_writers + [p for p in pages_years if p not in pages_writers]\n",
    "\n",
    "\n",
    "years = [int(re.search('\\d+', c.title).group()) for c in cat_years]\n",
    "\n",
    "print(f\"There are {len(pages_all)} wikipedia pages (novels) written between {min(years)} and {max(years)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b93bc08e04db882cea2ec055b69f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the text of the wikipedia pages.\n",
    "# Only sections containing the following keywords are considered:\n",
    "\n",
    "kws = ['plot', 'summary', 'topic', 'theme', 'summari',\n",
    "      'background', 'origin', 'introduction', 'concept', 'symbol',\n",
    "      'synopsis', 'content']\n",
    "\n",
    "# `documents` is a list of list of str, for each section of each document.\n",
    "documents = []\n",
    "for p in tqdm(pages_all):\n",
    "    documents.append(get_page_text(p, keywords=kws, verbose=False, use_summary_if_empty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "\n",
    "\n",
    "The text is processed for LDA:\n",
    "1. The text of each document is tokenized by words.\n",
    "2. Stopwords are removed. I use the set of stopwords of spaCy, to which I included additional words that occur frequently without being relevant (e.g. 'story', 'character', 'novel').\n",
    "3. Using the POS tagging feature of spaCy, only words with the allowed tag are kept. Here I keep nouns, verbs, adjectives, and adverbs.\n",
    "4. Words are reduce to their lemmas. (e.g. the lemma of 'went', 'gone', nd 'goes' is 'go')\n",
    "\n",
    "Some pages end up containing no tokens (if they have no relevant sections, or no sections at all). Those pages are removed from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(document,\n",
    "                         lemmatize=True,\n",
    "                         allowed_postags=None,\n",
    "                         min_len=3,\n",
    "                         additional_stopwords=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a list of tokens or lemmas for the text of `document`, filtering out stopwords.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    document (str).\n",
    "    lemmatize (bool): if True, returns a list of lemmas, otherwise a list of tokens.\n",
    "    allowed_postags (list of str): list of allowed POS tags. Tokens without an allowed POS tag are ignored.\n",
    "        If None, all POS tags are allowed.\n",
    "    min_len (int): tokens shorter than `min_len` (not included) are ignored.\n",
    "    additional_stopwords (list of str): list of stopwords to add to the defaults stopwords of spacy.\n",
    "        Note, the additional stopwords are also filtered out after lemmatization.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    list of str: list of valid tokens or lemmas.\n",
    "    \n",
    "    \"\"\"\n",
    "    if document == []:\n",
    "        return []\n",
    "    \n",
    "    tokens = []\n",
    "    for section in document:\n",
    "        tokens += [word for word in spacy_nlp(section) if\n",
    "                   (len(word.text) >= min_len and\n",
    "                    not word.is_stop and\n",
    "                    word.text not in additional_stopwords)]\n",
    "\n",
    "    if allowed_postags is not None:\n",
    "        tokens = [t for t in tokens if t.pos_ in allowed_postags]\n",
    "    if lemmatize:\n",
    "        tokens = [t.lemma_.lower() for t in tokens]\n",
    "    else:\n",
    "        tokens = [t.text.lower() for t in tokens]\n",
    "    return [t for t in tokens if t not in additional_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9b09ebeb034a0bbd02472532ffaffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 pages were ignored (they ended up empty after processing).\n",
      "There are 4845 documents (wikipedia pages).\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords=['story', 'character', 'novel', 'book', 'write',\n",
    "                      'writer', 'fiction', 'series', 'publish', 'year',\n",
    "                      'television', 'feature', 'american', 'british', 'narrator',\n",
    "                      'original', 'reference', 'author', 'chapter', 'film',\n",
    "                      'episode', 'release']\n",
    "\n",
    "indices_empty_documents = []\n",
    "tokenized_data = []\n",
    "\n",
    "for i, doc in tqdm(list(enumerate(documents))):\n",
    "    lda_tokens = prepare_text_for_lda(\n",
    "        doc,\n",
    "        lemmatize=True,\n",
    "        allowed_postags=['NOUN', 'VERB'],\n",
    "#         allowed_postags=['NOUN', 'VERB', 'PROPN'],\n",
    "        min_len=3,\n",
    "        # additional stopwords (after lemmatization)\n",
    "        additional_stopwords=additional_stopwords,\n",
    "    )\n",
    "    if len(lda_tokens) == 0:\n",
    "        indices_empty_documents.append(i)\n",
    "    tokenized_data.append(lda_tokens)\n",
    "print(f\"{len(indices_empty_documents)} pages were ignored (they ended up empty after processing).\")\n",
    "\n",
    "# remove pages without any valid tokens\n",
    "pages = [p for i, p in enumerate(pages_all) if i not in indices_empty_documents]\n",
    "tokenized_data = [t for i, t in enumerate(tokenized_data) if i not in indices_empty_documents]\n",
    "print(f'There are {len(tokenized_data)} documents (wikipedia pages).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Dictionary - associate a numeric id to each word\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form (bag-of-words)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional text mining\n",
    "\n",
    "Finally, I mine the wikipedia pages in order to obtain information about the __author__ of each page.\n",
    "\n",
    "I did not find any easy way of doing this with the wrapper, so I directly use the wikipedia API to extract the information in the __infoboxes__ of the wikipedia pages (box on the topright part of a page).\n",
    "Currently, if a page does not have the name of author in its infobox, or if it does not contain an infobox, the author is left unspecified.\n",
    "\n",
    "(This could be improved by mining the information in the main text of the page.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5f68c13aef4e609c7a6d160bf0b4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 novels have an unknown author.\n"
     ]
    }
   ],
   "source": [
    "# The wikipedia API accepts a maximum of 50 pageids at a time \n",
    "N = 50\n",
    "        \n",
    "authors = []\n",
    "for m in tqdm(range(len(pages)//N)):\n",
    "    authors += get_authors(pages[N*m:N*(m+1)])\n",
    "authors += get_authors(pages[N*(len(pages)//N):])\n",
    "\n",
    "doc2author = dict([(i, [author]) for i, author in enumerate(authors)])\n",
    "\n",
    "# replace the 'NA' tag for unknown authors with 'unknown_i', different for each novel.\n",
    "i = 0\n",
    "for key, value in doc2author.items():\n",
    "    if value == ['NA']:\n",
    "        doc2author[key] = ['unknown_' + str(i)]\n",
    "        i += 1\n",
    "        \n",
    "print(i, 'novels have an unknown author.')\n",
    "    \n",
    "# remove unknown authors (indicated by 'NA')\n",
    "# doc2author = {key: [elem for elem in value if elem != 'NA'] for key, value in doc2author.items()}\n",
    "author2doc = construct_author2doc(doc2author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/author2doc.json\", 'w') as f:\n",
    "        json.dump(author2doc, f, indent=2)\n",
    "        \n",
    "with open(\"data/tokenized_data.json\", 'w') as f:\n",
    "    json.dump(tokenized_data, f, indent=2)\n",
    "    \n",
    "dictionary.save('data/dictionary')\n",
    "corpora.MmCorpus.serialize('data/corpus.mm', corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

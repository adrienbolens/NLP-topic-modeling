{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorymembers(category, level=0, max_level=0, verbose=False, pages=None):\n",
    "    \"\"\"\n",
    "    Return a list of all wikipedia pages from a given category.\n",
    "    Categories are themselves pages -> The list includes pages from subcategories that\n",
    "    are members of a higher level category (recursively).\n",
    "    \n",
    "    NOTE: duplicates are not removed.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        category (str)\n",
    "        level (int): current level of the category\n",
    "        max_level (int): maximum level of the recursion for subcategories\n",
    "        verbose (bool)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        pages (list): list of `page`.\n",
    "    \n",
    "    \"\"\"\n",
    "    if pages is None:\n",
    "        pages = []\n",
    "    members = category.categorymembers\n",
    "    for page in members.values():\n",
    "        info_str = \"{0:s}: {1:s} (ns: {2:d})\".format('*' * (level + 1), page.title, page.ns)\n",
    "        if page.ns == wikipediaapi.Namespace.MAIN:\n",
    "            pages.append(page)\n",
    "            if verbose:\n",
    "                print(\"{0:70.70}{1:>27}\".format(info_str, 'PAGE ADDED'))\n",
    "        elif page.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            if verbose:\n",
    "                print(\"(SUBCATEGORY) \", info_str)\n",
    "            get_categorymembers(page, level + 1, verbose=verbose, pages = pages)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SUBCATEGORY)  *: Category:Mystery novels by Isaac Asimov (ns: 14)\n",
      "**: The Caves of Steel (ns: 0)                                                         PAGE ADDED\n",
      "**: The Death Dealers (ns: 0)                                                          PAGE ADDED\n",
      "**: Murder at the ABA (ns: 0)                                                          PAGE ADDED\n",
      "**: The Naked Sun (ns: 0)                                                              PAGE ADDED\n",
      "**: The Robots of Dawn (ns: 0)                                                         PAGE ADDED\n",
      "(SUBCATEGORY)  *: Category:Science fiction novels by Isaac Asimov (ns: 14)\n",
      "**: The Caves of Steel (ns: 0)                                                         PAGE ADDED\n",
      "**: The Currents of Space (ns: 0)                                                      PAGE ADDED\n",
      "**: David Starr, Space Ranger (ns: 0)                                                  PAGE ADDED\n",
      "**: The End of Eternity (ns: 0)                                                        PAGE ADDED\n",
      "**: Fantastic Voyage (novel) (ns: 0)                                                   PAGE ADDED\n",
      "**: Fantastic Voyage II: Destination Brain (ns: 0)                                     PAGE ADDED\n",
      "**: Forward the Foundation (ns: 0)                                                     PAGE ADDED\n",
      "**: Foundation (Asimov novel) (ns: 0)                                                  PAGE ADDED\n",
      "**: Foundation and Earth (ns: 0)                                                       PAGE ADDED\n",
      "**: Foundation and Empire (ns: 0)                                                      PAGE ADDED\n",
      "**: Foundation's Edge (ns: 0)                                                          PAGE ADDED\n",
      "**: The Gods Themselves (ns: 0)                                                        PAGE ADDED\n",
      "**: Lucky Starr and the Big Sun of Mercury (ns: 0)                                     PAGE ADDED\n",
      "**: Lucky Starr and the Moons of Jupiter (ns: 0)                                       PAGE ADDED\n",
      "**: Lucky Starr and the Oceans of Venus (ns: 0)                                        PAGE ADDED\n",
      "**: Lucky Starr and the Pirates of the Asteroids (ns: 0)                               PAGE ADDED\n",
      "**: Lucky Starr and the Rings of Saturn (ns: 0)                                        PAGE ADDED\n",
      "**: Lucky Starr series (ns: 0)                                                         PAGE ADDED\n",
      "**: The Naked Sun (ns: 0)                                                              PAGE ADDED\n",
      "**: Nemesis (Asimov novel) (ns: 0)                                                     PAGE ADDED\n",
      "**: Nightfall (Asimov novelette and novel) (ns: 0)                                     PAGE ADDED\n",
      "**: Norby, the Mixed-Up Robot (ns: 0)                                                  PAGE ADDED\n",
      "**: Pebble in the Sky (ns: 0)                                                          PAGE ADDED\n",
      "**: The Positronic Man (ns: 0)                                                         PAGE ADDED\n",
      "**: Prelude to Foundation (ns: 0)                                                      PAGE ADDED\n",
      "**: Robots and Empire (ns: 0)                                                          PAGE ADDED\n",
      "**: The Robots of Dawn (ns: 0)                                                         PAGE ADDED\n",
      "**: Second Foundation (ns: 0)                                                          PAGE ADDED\n",
      "**: The Stars, Like Dust (ns: 0)                                                       PAGE ADDED\n",
      "**: The Ugly Little Boy (ns: 0)                                                        PAGE ADDED\n"
     ]
    }
   ],
   "source": [
    "# cat = wiki_wiki.page(\"Category:Science fiction novels by writer\")\n",
    "cat = wiki_wiki.page(\"Category:Novels by Isaac Asimov\")\n",
    "pages = get_categorymembers(cat, max_level=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import models, corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plot introduction', 'Plot summary', 'Characters']\n",
      "['Plot summary', 'Background science', 'Characters']\n",
      "['Origins', 'Plot outline', 'Characters']\n",
      "['Plot']\n",
      "['Plot summary', 'Characters']\n",
      "['Plot introduction', 'Plot summary', 'Characters']\n",
      "['Plot summary']\n",
      "['Plot summary', 'Themes']\n",
      "['Plot', 'Concepts', 'Major characters', 'Origins']\n",
      "['Plot', 'Similarly themed works']\n",
      "['Conception', 'Plot']\n",
      "['Plot']\n",
      "['Origin and early publication history',\n",
      " 'Background',\n",
      " 'Plot summary',\n",
      " 'Characters']\n",
      "['Plot introduction', 'Plot summary']\n",
      "['Plot summary', 'Characters']\n",
      "['Plot summary']\n",
      "['Plot summary']\n",
      "['Plot summary', 'Themes']\n",
      "['Plot summary', 'Themes']\n",
      "['Plot summary', 'Themes']\n",
      "['Plot summary', 'Themes']\n",
      "['Plot summary', 'Themes']\n",
      "[]\n",
      "['Plot']\n",
      "['Plot summary', 'Major characters']\n",
      "['Background', 'Plot summary']\n",
      "['Plot summary']\n",
      "['Story background', 'Plot summary']\n",
      "['Plot summary']\n",
      "['Plot summary', 'Characters']\n",
      "['Plot summary']\n",
      "['Plot summary', 'Characters']\n",
      "['Plot summary']\n",
      "['Plot']\n",
      "['Plot summary', 'Original ending']\n"
     ]
    }
   ],
   "source": [
    "def check_section_title(title):\n",
    "    return bool(re.search('plot|'\n",
    "                          'character|'\n",
    "                          'summary|'\n",
    "                          'topic|'\n",
    "                          'theme|'\n",
    "                          'summari|'\n",
    "                          'background|'\n",
    "                          'origin|'\n",
    "                          'introduction|'\n",
    "                          'concept|'\n",
    "                          'symbol', title.lower()))\n",
    "                \n",
    "for p in pages:\n",
    "    pprint([s.title for s in p.sections if check_section_title(s.title)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(set(STOPWORDS) - set(stopwords.words('english'))))\n",
    "print(len(set(stopwords.words('english')) - set(STOPWORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS_ALL = set(STOPWORDS).union(set(stopwords.words('english')))\n",
    "print(len(STOPWORDS_ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# import spacy\n",
    "# spacy.load('en')\n",
    "# from spacy.lang.en import English\n",
    "# parser = English()\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "# lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [t for t in tokens if (\n",
    "        t not in STOPWORDS_ALL and\n",
    "        len(t) > 2)]\n",
    "#         re.match('[a-zA-Z\\-]{3,}', t)\n",
    "#         re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)\n",
    "#     )]\n",
    "    token_lemmas = [get_lemma(token) for token in clean_tokens]\n",
    "    return token_lemmas\n",
    "\n",
    "def clean_text2(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "            \n",
    "    token_lemmas = [get_lemma(token) for token in lda_tokens]\n",
    "#     return lda_tokens \n",
    "    return token_lemmas\n",
    "\n",
    "def select_and_clean_text(page):\n",
    "    tokens = []\n",
    "    for section in page.sections:\n",
    "#         if check_section_title(section.title):\n",
    "        if True:\n",
    "            tokens += clean_text(section.text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cave', 'steel', 'science', 'fiction', 'novel', 'american', 'writer', 'isaac', 'asimov', 'detective']\n",
      "['novel', 'isaac', 'asimov', 'introduce', 'elijah', 'baley', 'daneel', 'olivaw', 'later', 'favorite']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(pages[0].text)[:10])\n",
    "\n",
    "# print(pages[0].text)\n",
    "# print(pages[0].sections[0])\n",
    "print(select_and_clean_text(pages[0])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'corpus', 'better', None]\n",
      "['rock', 'corpus', 'better', 'asdasdasd']\n"
     ]
    }
   ],
   "source": [
    "# print([lemmatizer.lemmatize(s) for s in ['rocks', 'corpora', 'better', 'asdasdasd']])\n",
    "print([wn.morphy(s) for s in ['rocks', 'corpora', 'better', 'asdasdasd']])\n",
    "print([get_lemma(s) for s in ['rocks', 'corpora', 'better', 'asdasdasd']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for p in pages:\n",
    "    tokenized_data.append(select_and_clean_text(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 3), (18, 1), (19, 2)]\n",
      "['able', 'actually', 'adapt']\n"
     ]
    }
   ],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "id2word = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [id2word.doc2bow(text) for text in tokenized_data]\n",
    " \n",
    "# Have a look at how the 4th document looks like: [(word_id, count), ...]\n",
    "# (only the first three words)\n",
    "example_corpus = corpus[3][:3]\n",
    "print(example_corpus)\n",
    "print([id2word[i] for i, _ in example_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "[\"'caves\", \"'how\", \"'olden\", \"'open\", \"'ve\", \"'what\", '1950s', '1953', '1954', '1964', '1989', '2004', '2016', 'able', 'accept', 'accident', 'accidentally', 'accomplish', 'actually', 'adapt', 'adaptation', 'advance', 'advice', 'ago', 'agoraphobia', 'agree', 'air', 'akiva', 'ambassador', 'answer', 'antagonism', 'anthony', 'anti-robot', 'apartment', 'appearance', 'archive', 'arrest', 'asimov', 'asimovian', 'aspect', 'assign', 'astonishment', 'attempt', 'attribute', 'aurora', 'average', 'award', 'away', 'background', 'baley', 'barrett', 'bbc', 'begin', 'beginning', 'behaviour', 'belief', 'bentley', 'bert', 'best', 'biblical', 'billion', 'birth', 'bishop', 'blaster', 'block', 'book', 'boucher', 'brain', 'bring', 'bureaucratically', 'call', 'campbell', 'capable', 'case', 'cause', 'cave', 'central', 'character', 'charge', 'chest', 'city', 'claustrophile', 'clear', 'close', 'clousarr', 'colonization', 'colonize', 'colonized—fifty', 'combination', 'combine', 'come', 'comfortably', 'commissioner', 'complex', 'concert', 'conklin', 'connect', 'consider', 'consume', 'continue']\n"
     ]
    }
   ],
   "source": [
    "print(id2word.doc2bow(tokenized_data[0])[:5])\n",
    "print([id2word[i] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=id2word)\n",
    " \n",
    "# Build the LSI model\n",
    "# lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99767035)]\n"
     ]
    }
   ],
   "source": [
    "text = tokenized_data[1]\n",
    "bow = id2word.doc2bow(text)\n",
    " \n",
    "# print(lsi_model[bow])\n",
    "print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "[(0,\n",
      "  '0.009*\"baley\" + 0.008*\"earth\" + 0.008*\"starr\" + 0.007*\"robot\" + '\n",
      "  '0.006*\"asimov\" + 0.005*\"novel\" + 0.005*\"foundation\" + 0.004*\"time\" + '\n",
      "  '0.003*\"world\" + 0.003*\"write\"'),\n",
      " (1,\n",
      "  '0.009*\"asimov\" + 0.008*\"robot\" + 0.008*\"baley\" + 0.007*\"starr\" + '\n",
      "  '0.006*\"novel\" + 0.006*\"earth\" + 0.006*\"story\" + 0.004*\"time\" + 0.004*\"book\" '\n",
      "  '+ 0.004*\"foundation\"'),\n",
      " (2,\n",
      "  '0.009*\"robot\" + 0.009*\"asimov\" + 0.008*\"starr\" + 0.008*\"novel\" + '\n",
      "  '0.006*\"book\" + 0.006*\"earth\" + 0.005*\"story\" + 0.005*\"foundation\" + '\n",
      "  '0.004*\"fiction\" + 0.004*\"seldon\"')]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    "\n",
    "# Print the Keyword in all topics\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "print(\"=\" * 20)\n",
    " \n",
    "# print(\"LSI Model:\")\n",
    " \n",
    "# pprint(lsi_model.print_topics())\n",
    "# doc_lsi = lsi_model[corpus]\n",
    "    \n",
    "# print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 0.99895793)], [(0, 0.9976705)], [(2, 0.9971428)], [(1, 0.010094799), (2, 0.98915887)], [(1, 0.9987263)], [(0, 0.99895805)], [(2, 0.9979732)], [(0, 0.99893624)], [(2, 0.9916454)], [(1, 0.015623452), (2, 0.9840673)], [(1, 0.087184444), (2, 0.91129506)], [(2, 0.99629533)], [(2, 0.99669194)], [(0, 0.82272005), (1, 0.026507914), (2, 0.15077206)], [(2, 0.99412894)], [(0, 0.026046062), (1, 0.13702397), (2, 0.83693)], [(1, 0.9687337), (2, 0.030332271)], [(2, 0.9985265)], [(2, 0.99762684)], [(2, 0.9964418)], [(1, 0.9987057)], [(2, 0.9988942)], [(2, 0.9966055)], [(2, 0.99020594)], [(0, 0.9980357)], [(1, 0.3650811), (2, 0.6344041)], [(2, 0.9969212)], [(2, 0.9989096)], [(2, 0.9966111)], [(2, 0.9985781)], [(1, 0.90264976), (2, 0.09663812)], [(1, 0.9987262)], [(2, 0.9969156)], [(1, 0.0122757405), (2, 0.9870892)], [(1, 0.9984863)]]\n"
     ]
    }
   ],
   "source": [
    "print([a for a in doc_lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9926275)]\n",
      "[(2, 0.99147034)]\n",
      "[(1, 0.011284761), (2, 0.9879689)]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print([a for a in lda_model[corpus][i]])\n",
    "print([a for a in lda_model[corpus[i]]])\n",
    "print([a for a in lda_model[corpus]][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [p.title for p in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0], 'The Caves of Steel'),\n",
       " ([0], 'The Death Dealers'),\n",
       " ([2], 'Murder at the ABA'),\n",
       " ([2], 'The Naked Sun'),\n",
       " ([1], 'The Robots of Dawn'),\n",
       " ([0], 'The Caves of Steel'),\n",
       " ([2], 'The Currents of Space'),\n",
       " ([0], 'David Starr, Space Ranger'),\n",
       " ([2], 'The End of Eternity'),\n",
       " ([1, 2], 'Fantastic Voyage'),\n",
       " ([1, 2], 'Fantastic Voyage II: Destination Brain'),\n",
       " ([2], 'Forward the Foundation'),\n",
       " ([2], 'Foundation (Asimov novel)'),\n",
       " ([0, 1, 2], 'Foundation and Earth'),\n",
       " ([2], 'Foundation and Empire'),\n",
       " ([0, 1, 2], \"Foundation's Edge\"),\n",
       " ([1, 2], 'The Gods Themselves'),\n",
       " ([2], 'Lucky Starr and the Big Sun of Mercury'),\n",
       " ([2], 'Lucky Starr and the Moons of Jupiter'),\n",
       " ([2], 'Lucky Starr and the Oceans of Venus'),\n",
       " ([1], 'Lucky Starr and the Pirates of the Asteroids'),\n",
       " ([2], 'Lucky Starr and the Rings of Saturn'),\n",
       " ([2], 'Lucky Starr series'),\n",
       " ([1, 2], 'The Naked Sun'),\n",
       " ([0], 'Nemesis (Asimov novel)'),\n",
       " ([1, 2], 'Nightfall (Asimov novelette and novel)'),\n",
       " ([2], 'Norby, the Mixed-Up Robot'),\n",
       " ([2], 'Pebble in the Sky'),\n",
       " ([2], 'The Positronic Man'),\n",
       " ([2], 'Prelude to Foundation'),\n",
       " ([1, 2], 'Robots and Empire'),\n",
       " ([1], 'The Robots of Dawn'),\n",
       " ([2], 'Second Foundation'),\n",
       " ([1, 2], 'The Stars, Like Dust'),\n",
       " ([1], 'The Ugly Little Boy')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([[i for i, _ in a] for a in list(doc_lda)], titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0), (10, 1.0), (22, 1.0), (33, 1.0), (14, 0.57735026), (32, 0.57735026), (8, 0.32445747), (16, 0.30843654), (6, 0.08668428), (13, 0.011009008), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (7, 0.0), (9, 0.0), (11, 0.0), (12, 0.0), (15, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (34, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    " \n",
    "# basically the calculate the cosine of a given vector with the vectors in lda_model[corpus]\n",
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    " \n",
    "# Let's perform some queries\n",
    "similarities = lda_index[lda_model[bow]]\n",
    "# Sort the similarities\n",
    "similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    " \n",
    "# Top most similar documents:\n",
    "print(similarities)\n",
    "# It should be itself...\n",
    "\n",
    "\n",
    "# Let's see what's the most similar document\n",
    "document_id, similarity = similarities[0]\n",
    "# print(pages[document_id].text[:1000])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-07671fdee36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adrienbolens/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 2:\n",
    "#             result.append(lemmatize_stemming(token))\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess(pages[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='aad'>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', 'aad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

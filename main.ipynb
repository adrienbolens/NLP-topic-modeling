{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorymembers(category, level=0, max_level=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Return a list of all wikipedia pages from a given category.\n",
    "    Categories are themselves pages -> The list includes pages from subcategories that\n",
    "    are members of a higher level category (recursively).\n",
    "    \n",
    "    NOTE: duplicates are not removed.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        category (str)\n",
    "        level (int): current level of the category\n",
    "        max_level (int): maximum level of the recursion for subcategories\n",
    "        verbose (bool)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        pages (list): list of `page`.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pages = []\n",
    "    members = category.categorymembers\n",
    "    for page in members.values():\n",
    "        info_str = \"{0:s}: {1:s} (ns: {2:d})\".format('*' * (level + 1), page.title, page.ns)\n",
    "        if page.ns == wikipediaapi.Namespace.MAIN:\n",
    "            pages.append(page)\n",
    "            if verbose:\n",
    "                print(\"{0:70.70}{1:>27}\".format(info_str, 'PAGE ADDED'))\n",
    "        if page.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            if verbose:\n",
    "                print(\"(SUBCATEGORY) \", info_str)\n",
    "            get_categorymembers(page, level + 1)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*: Dirk Gently's Holistic Detective Agency (ns: 0)                                     PAGE ADDED\n",
      "*: The Hitchhiker's Guide to the Galaxy (ns: 0)                                        PAGE ADDED\n",
      "*: The Hitchhiker's Guide to the Galaxy (novel) (ns: 0)                                PAGE ADDED\n",
      "*: Life, the Universe and Everything (ns: 0)                                           PAGE ADDED\n",
      "*: The Long Dark Tea-Time of the Soul (ns: 0)                                          PAGE ADDED\n",
      "*: Mostly Harmless (ns: 0)                                                             PAGE ADDED\n",
      "*: The Restaurant at the End of the Universe (ns: 0)                                   PAGE ADDED\n",
      "*: So Long, and Thanks for All the Fish (ns: 0)                                        PAGE ADDED\n"
     ]
    }
   ],
   "source": [
    "# cat = wiki_wiki.page(\"Category:Science fiction novels by writer\")\n",
    "cat = wiki_wiki.page(\"Category:Novels by Douglas Adams\")\n",
    "pages = get_categorymembers(cat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import models, corpora\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(set(STOPWORDS) - set(stopwords.words('english'))))\n",
    "print(len(set(stopwords.words('english')) - set(STOPWORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS_ALL = set(STOPWORDS).union(set(stopwords.words('english')))\n",
    "print(len(STOPWORDS_ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [t for t in tokens if (\n",
    "        t not in STOPWORDS_ALL and\n",
    "        re.match('[a-zA-Z\\-]{3,}', t)\n",
    "#         re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)\n",
    "    )]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for p in pages:\n",
    "    tokenized_data.append(clean_text(p.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 2), (16, 4), (17, 1), (18, 1), (21, 1), (22, 1), (34, 1), (45, 1), (46, 1), (50, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    " \n",
    "# Have a look at how the 4th document looks like: [(word_id, count), ...]\n",
    "# (only the first 10 words)\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 6\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    " \n",
    "# Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic #0: 0.014*\"series\" + 0.013*\"adams\" + 0.012*\"guide\" + 0.010*\"arthur\" + 0.009*\"hitchhiker\"\n",
      "Topic #1: 0.013*\"series\" + 0.009*\"adams\" + 0.008*\"book\" + 0.008*\"arthur\" + 0.007*\"radio\"\n",
      "Topic #2: 0.010*\"arthur\" + 0.009*\"series\" + 0.007*\"adams\" + 0.007*\"radio\" + 0.006*\"universe\"\n",
      "Topic #3: 0.011*\"series\" + 0.010*\"arthur\" + 0.008*\"radio\" + 0.008*\"hitchhiker\" + 0.008*\"adams\"\n",
      "Topic #4: 0.016*\"series\" + 0.011*\"guide\" + 0.009*\"hitchhiker\" + 0.009*\"adams\" + 0.008*\"radio\"\n",
      "Topic #5: 0.011*\"radio\" + 0.008*\"series\" + 0.008*\"book\" + 0.007*\"guide\" + 0.006*\"hitchhiker\"\n",
      "====================\n",
      "LSI Model:\n",
      "Topic #0: 0.419*\"series\" + 0.312*\"radio\" + 0.280*\"guide\" + 0.258*\"hitchhiker\" + 0.223*\"adams\"\n",
      "Topic #1: -0.317*\"gently\" + -0.289*\"dirk\" + -0.208*\"adams\" + -0.195*\"macduff\" + -0.182*\"ghost\"\n",
      "Topic #2: -0.505*\"arthur\" + -0.171*\"krikkit\" + 0.164*\"radio\" + -0.153*\"robots\" + 0.146*\"series\"\n",
      "Topic #3: -0.246*\"universe\" + -0.236*\"krikkit\" + 0.230*\"earth\" + -0.201*\"robots\" + 0.182*\"fish\"\n",
      "Topic #4: 0.370*\"zaphod\" + 0.270*\"zarniwoop\" + 0.180*\"ford\" + -0.174*\"fenchurch\" + 0.168*\"ship\"\n",
      "Topic #5: -0.279*\"zaphod\" + -0.215*\"zarniwoop\" + 0.189*\"random\" + -0.161*\"fenchurch\" + 0.146*\"harmless\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 5))\n",
    "    \n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 5))\n",
    "    \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 36.92539958217678), (1, -73.92019664694953), (2, 2.8143817661235544), (3, -0.9842033820394213), (4, 1.153303105860656), (5, 0.4581425670460037)]\n",
      "[(1, 0.39767718), (4, 0.6018241)]\n"
     ]
    }
   ],
   "source": [
    "text = pages[0].text\n",
    "bow = dictionary.doc2bow(clean_text(text))\n",
    " \n",
    "print(lsi_model[bow])\n",
    "print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99997854), (4, 0.8601553), (2, 0.12233368), (6, 0.027768016), (3, 0.020850312), (1, 0.0), (5, 0.0), (7, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    " \n",
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    " \n",
    "# Let's perform some queries\n",
    "similarities = lda_index[lda_model[bow]]\n",
    "# Sort the similarities\n",
    "similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    " \n",
    "# Top most similar documents:\n",
    "print(similarities)\n",
    "# [(104, 0.87591344), (178, 0.86124849), (31, 0.8604598), (77, 0.84932965), (85, 0.84843522), (135, 0.84421808), (215, 0.84184396), (353, 0.84038532), (254, 0.83498049), (13, 0.82832891)]\n",
    " \n",
    "# Let's see what's the most similar document\n",
    "document_id, similarity = similarities[0]\n",
    "# print(pages[document_id].text[:1000])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9993975 , 0.        , 0.12024248, 0.02183226, 0.84545153,\n",
       "       0.        , 0.02907575, 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_index[lda_model[bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 2:\n",
    "#             result.append(lemmatize_stemming(token))\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess(pages[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='aad'>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', 'aad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

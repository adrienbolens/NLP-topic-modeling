{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorymembers(category, level=0, max_level=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Return a list of all wikipedia pages from a given category.\n",
    "    Categories are themselves pages -> The list includes pages from subcategories that\n",
    "    are members of a higher level category (recursively).\n",
    "    \n",
    "    NOTE: duplicates are not removed.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        category (str)\n",
    "        level (int): current level of the category\n",
    "        max_level (int): maximum level of the recursion for subcategories\n",
    "        verbose (bool)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        pages (list): list of `page`.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pages = []\n",
    "    members = category.categorymembers\n",
    "    for page in members.values():\n",
    "        info_str = \"{0:s}: {1:s} (ns: {2:d})\".format('*' * (level + 1), page.title, page.ns)\n",
    "        if page.ns == wikipediaapi.Namespace.MAIN:\n",
    "            pages.append(page)\n",
    "            if verbose:\n",
    "                print(\"{0:70.70}{1:>27}\".format(info_str, 'PAGE ADDED'))\n",
    "        if page.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            if verbose:\n",
    "                print(\"(SUBCATEGORY) \", info_str)\n",
    "            get_categorymembers(page, level + 1)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*: Dirk Gently's Holistic Detective Agency (ns: 0)                                     PAGE ADDED\n",
      "*: The Hitchhiker's Guide to the Galaxy (ns: 0)                                        PAGE ADDED\n",
      "*: The Hitchhiker's Guide to the Galaxy (novel) (ns: 0)                                PAGE ADDED\n",
      "*: Life, the Universe and Everything (ns: 0)                                           PAGE ADDED\n",
      "*: The Long Dark Tea-Time of the Soul (ns: 0)                                          PAGE ADDED\n",
      "*: Mostly Harmless (ns: 0)                                                             PAGE ADDED\n",
      "*: The Restaurant at the End of the Universe (ns: 0)                                   PAGE ADDED\n",
      "*: So Long, and Thanks for All the Fish (ns: 0)                                        PAGE ADDED\n"
     ]
    }
   ],
   "source": [
    "# cat = wiki_wiki.page(\"Category:Science fiction novels by writer\")\n",
    "cat = wiki_wiki.page(\"Category:Novels by Douglas Adams\")\n",
    "pages = get_categorymembers(cat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import models, corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(set(STOPWORDS) - set(stopwords.words('english'))))\n",
    "print(len(set(stopwords.words('english')) - set(STOPWORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS_ALL = set(STOPWORDS).union(set(stopwords.words('english')))\n",
    "print(len(STOPWORDS_ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [t for t in tokens if (\n",
    "        t not in STOPWORDS_ALL and\n",
    "        re.match('[a-zA-Z\\-]{3,}', t)\n",
    "#         re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)\n",
    "    )]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "for p in pages:\n",
    "    tokenized_data.append(clean_text(p.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 2), (16, 4), (17, 1), (18, 1), (21, 1), (22, 1), (34, 1), (45, 1), (46, 1), (50, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    " \n",
    "# Have a look at how the 4th document looks like: [(word_id, count), ...]\n",
    "# (only the first 10 words)\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abend'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 6\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    " \n",
    "# Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 240.37361948551592), (1, 11.754043740631356), (2, 10.998284135263896), (3, -1.7140595870982118), (4, -1.337397510248843), (5, 0.3180635755088952)]\n",
      "[(1, 0.9990717)]\n"
     ]
    }
   ],
   "source": [
    "text = tokenized_data[1]\n",
    "bow = dictionary.doc2bow(text)\n",
    " \n",
    "print(lsi_model[bow])\n",
    "print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "[(0,\n",
      "  '0.012*\"series\" + 0.010*\"guide\" + 0.010*\"adams\" + 0.008*\"radio\" + '\n",
      "  '0.008*\"hitchhiker\" + 0.008*\"arthur\" + 0.007*\"book\" + 0.007*\"earth\" + '\n",
      "  '0.006*\"universe\" + 0.005*\"novel\"'),\n",
      " (1,\n",
      "  '0.014*\"series\" + 0.012*\"guide\" + 0.010*\"arthur\" + 0.009*\"radio\" + '\n",
      "  '0.009*\"hitchhiker\" + 0.009*\"adams\" + 0.009*\"galaxy\" + 0.008*\"book\" + '\n",
      "  '0.007*\"earth\" + 0.005*\"zaphod\"'),\n",
      " (2,\n",
      "  '0.012*\"series\" + 0.010*\"adams\" + 0.009*\"book\" + 0.007*\"hitchhiker\" + '\n",
      "  '0.007*\"radio\" + 0.007*\"guide\" + 0.006*\"arthur\" + 0.006*\"universe\" + '\n",
      "  '0.004*\"earth\" + 0.004*\"dirk\"'),\n",
      " (3,\n",
      "  '0.011*\"arthur\" + 0.009*\"series\" + 0.008*\"hitchhiker\" + 0.007*\"radio\" + '\n",
      "  '0.007*\"book\" + 0.007*\"adams\" + 0.006*\"guide\" + 0.006*\"earth\" + 0.006*\"ford\" '\n",
      "  '+ 0.005*\"universe\"'),\n",
      " (4,\n",
      "  '0.014*\"series\" + 0.010*\"arthur\" + 0.010*\"adams\" + 0.009*\"radio\" + '\n",
      "  '0.008*\"guide\" + 0.007*\"universe\" + 0.006*\"book\" + 0.006*\"hitchhiker\" + '\n",
      "  '0.006*\"earth\" + 0.004*\"zaphod\"'),\n",
      " (5,\n",
      "  '0.010*\"series\" + 0.009*\"adams\" + 0.008*\"radio\" + 0.008*\"book\" + '\n",
      "  '0.007*\"hitchhiker\" + 0.007*\"arthur\" + 0.007*\"guide\" + 0.005*\"galaxy\" + '\n",
      "  '0.004*\"earth\" + 0.004*\"dirk\"')]\n",
      "====================\n",
      "LSI Model:\n",
      "[(0,\n",
      "  '0.419*\"series\" + 0.312*\"radio\" + 0.280*\"guide\" + 0.258*\"hitchhiker\" + '\n",
      "  '0.223*\"adams\" + 0.200*\"book\" + 0.176*\"arthur\" + 0.152*\"galaxy\" + '\n",
      "  '0.139*\"earth\" + 0.137*\"universe\"'),\n",
      " (1,\n",
      "  '-0.317*\"gently\" + -0.289*\"dirk\" + -0.208*\"adams\" + -0.195*\"macduff\" + '\n",
      "  '-0.182*\"chronotis\" + -0.182*\"ghost\" + -0.182*\"monk\" + -0.172*\"richard\" + '\n",
      "  '-0.149*\"detective\" + -0.143*\"way\"'),\n",
      " (2,\n",
      "  '-0.505*\"arthur\" + -0.171*\"krikkit\" + 0.164*\"radio\" + -0.153*\"robots\" + '\n",
      "  '0.146*\"series\" + -0.145*\"planet\" + -0.143*\"universe\" + -0.141*\"earth\" + '\n",
      "  '-0.137*\"ford\" + -0.116*\"fenchurch\"'),\n",
      " (3,\n",
      "  '-0.246*\"universe\" + -0.236*\"krikkit\" + 0.230*\"earth\" + -0.201*\"robots\" + '\n",
      "  '0.182*\"fish\" + 0.182*\"guide\" + 0.182*\"fenchurch\" + 0.147*\"random\" + '\n",
      "  '0.137*\"harmless\" + -0.135*\"hactar\"'),\n",
      " (4,\n",
      "  '0.370*\"zaphod\" + 0.270*\"zarniwoop\" + 0.180*\"ford\" + -0.174*\"fenchurch\" + '\n",
      "  '0.168*\"ship\" + 0.158*\"universe\" + -0.142*\"thanks\" + 0.131*\"guide\" + '\n",
      "  '0.131*\"random\" + -0.125*\"long\"'),\n",
      " (5,\n",
      "  '-0.279*\"zaphod\" + -0.215*\"zarniwoop\" + 0.189*\"random\" + -0.161*\"fenchurch\" '\n",
      "  '+ 0.146*\"harmless\" + -0.143*\"thanks\" + 0.139*\"guide\" + -0.131*\"universe\" + '\n",
      "  '0.113*\"tricia\" + -0.113*\"marvin\"')]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    "\n",
    "# Print the Keyword in all topics\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    " \n",
    "pprint(lsi_model.print_topics())\n",
    "doc_lsi = lsi_model[corpus]\n",
    "    \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1a26637650>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1.0), (5, 1.0), (7, 1.0), (2, 0.680604), (6, 0.40527242), (0, 0.0), (3, 0.0), (4, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    " \n",
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    " \n",
    "# Let's perform some queries\n",
    "similarities = lda_index[lda_model[bow]]\n",
    "# Sort the similarities\n",
    "similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    " \n",
    "# Top most similar documents:\n",
    "print(similarities)\n",
    "# [(104, 0.87591344), (178, 0.86124849), (31, 0.8604598), (77, 0.84932965), (85, 0.84843522), (135, 0.84421808), (215, 0.84184396), (353, 0.84038532), (254, 0.83498049), (13, 0.82832891)]\n",
    "\n",
    "# Let's see what's the most similar document\n",
    "document_id, similarity = similarities[0]\n",
    "# print(pages[document_id].text[:1000])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/bolensadrien/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 2:\n",
    "#             result.append(lemmatize_stemming(token))\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess(pages[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='aad'>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', 'aad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
